// api/audio-diagnose.js
import OpenAI, { toFile } from "openai";
import { findRelevantIssues } from "../lib/autoKnowledge.js";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Ù†ÙØ³ Ø¯Ø§Ù„Ø© ØªØ®Ù…ÙŠÙ† Ø§Ù„Ù„ØºØ© Ø§Ù„ØªÙŠ Ø§Ø³ØªØ®Ø¯Ù…Ù†Ø§Ù‡Ø§ ÙÙŠ image-diagnose
function guessLanguage(text) {
  if (!text || !text.trim()) return "en";
  const t = text.trim();

  if (/[\u0600-\u06FF]/.test(t)) return "ar"; // Arabic
  if (/[\u0400-\u04FF]/.test(t)) return "ru"; // Russian
  if (/[\u0370-\u03FF]/.test(t)) return "el"; // Greek
  if (/[\u4E00-\u9FFF]/.test(t)) return "zh"; // Chinese
  if (/[\u3040-\u30FF]/.test(t)) return "ja"; // Japanese
  if (/[\u1100-\u11FF]/.test(t)) return "ko"; // Korean

  return "en";
}

// ğŸ§  Ø¨Ø±ÙˆÙ…Ø¨Øª Ø¹Ø§Ù… Ù„ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø³ÙŠØ§Ø±Ø§Øª
const BASE_SYSTEM_PROMPT = `
You are **FixLens Auto**, an intelligent automotive diagnosis assistant.
You receive:
- A short **transcription of what the user said in the voice note** (could be any language).
- Optional **notes** from the user.
- Optional **matched common issues JSON** from auto_common_issues.json.

Your job:
1. Understand the described symptoms (noises, vibrations, leaks, warning lights, smells, performance issues, starting problems, etc.).
2. Combine:
   - The voice transcription,
   - Any extra user notes,
   - And the relevant issues JSON (if provided)
   to produce a high-quality, structured answer for a car owner or mechanic.

Always answer in the **same language as the user** (if it seems Arabic, answer Arabic; if English, answer English, etc.).

Your reply MUST follow this structure (markdown):

**Quick Summary:**
- ...

**Most Likely Causes:**
1. ...
2. ...

**What You Can Check Now:**
- ...

**Safety / When to Stop Driving:**
- ...

**Next Professional Step:**
- ...

If information is not enough, be honest and politely ask for more details instead of guessing blindly.
`;

// Ø¯Ø§Ù„Ø© ØµØºÙŠØ±Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† response Ù…Ù† Ù†ÙˆØ¹ OpenAI Responses API
function extractTextFromResponse(resp) {
  try {
    const first = resp.output?.[0];
    if (!first || !first.content) return null;

    const textPart = first.content.find((c) => c.type === "output_text");
    if (textPart && textPart.text) return textPart.text.toString();

    // fallback Ù‚Ø¯ÙŠÙ…
    if (typeof first.output_text === "string") return first.output_text;
  } catch (e) {
    console.error("Failed to extract text from OpenAI response:", e);
  }
  return null;
}

export default async function handler(req, res) {
  if (req.method !== "POST") {
    res.setHeader("Allow", ["POST"]);
    return res.status(405).json({ error: "Method not allowed" });
  }

  try {
    // ğŸ“¨ Ù†Ù‚Ø±Ø£ Ø§Ù„Ø¬Ø³Ù… (JSON) Ù…Ù† Flutter
    const body =
      typeof req.body === "string" ? JSON.parse(req.body) : req.body || {};

    const {
      audioBase64,
      mimeType,
      language: preferredLanguage, // "auto" Ø£Ùˆ "en" Ø£Ùˆ "ar" ...
      note, // Ù…Ù„Ø§Ø­Ø¸Ø© Ø¥Ø¶Ø§ÙÙŠØ© Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
      mode, // Ù…ØªØ±ÙˆÙƒ Ù„Ù„Ù…Ø³ØªÙ‚Ø¨Ù„
    } = body;

    if (!audioBase64) {
      return res
        .status(400)
        .json({ error: "Missing 'audioBase64' in request body." });
    }

    // ğŸ§Š Ù†Ø­ÙˆÙ„ Ø§Ù„Ù€ base64 Ø¥Ù„Ù‰ Buffer
    const audioBuffer = Buffer.from(audioBase64, "base64");

    // Ù†Ø®Ø¨Ø± toFile Ø¨Ø§Ø³Ù… Ùˆ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù (Whisper ÙŠØ¯Ø¹Ù… m4a, mp3, mp4, wav, webm, ogg, mpeg, mpga ...)
    const file = await toFile(
      audioBuffer,
      "recording.m4a", // Ø§Ù„Ø§Ø³Ù… ÙÙ‚Ø· â€“ Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯ Ù„Ø£ØºØ±Ø§Ø¶ Ø§Ù„ØªÙˆØ§ÙÙ‚
      {
        type: mimeType || "audio/m4a",
      }
    );

    // ğŸ§ Ø£ÙˆÙ„Ø§Ù‹: Ù†Ø¹Ù…Ù„ Transcription Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Whisper
    const transcription = await openai.audio.transcriptions.create({
      model: "whisper-1",
      file,
      // Ø¥Ø°Ø§ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø­Ø¯Ù‘Ø¯ Ø§Ù„Ù„ØºØ© ÙŠØ¯ÙˆÙŠÙ‹Ø§ ØºÙŠØ± "auto" Ù†Ù…Ø±Ø±Ù‡Ø§ØŒ ØºÙŠØ± Ø°Ù„Ùƒ Ù†Ø®Ù„ÙŠ Whisper ÙŠÙƒØªØ´Ù
      ...(preferredLanguage &&
      preferredLanguage !== "auto" &&
      typeof preferredLanguage === "string"
        ? { language: preferredLanguage }
        : {}),
    });

    const transcriptText = (transcription.text || "").trim();
    console.log("FixLens audio transcript:", transcriptText);

    // Ù†Ø­Ø¯Ø¯ Ù„ØºØ© Ø§Ù„Ø±Ø¯
    let finalLanguage =
      preferredLanguage && preferredLanguage !== "auto"
        ? preferredLanguage
        : guessLanguage(transcriptText || note || "");

    // ğŸ§© Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯ ÙƒÙ„Ø§Ù… ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ (ÙÙ‚Ø· Ø¶ÙˆØ¶Ø§Ø¡ / Ù…Ø­Ø±Ùƒ)ØŒ Ù†Ø±Ø¯ Ø¨Ø±Ø¯ Ø®Ø§Øµ Ù…Ø­ØªØ±Ù…
    if (!transcriptText || transcriptText.length < 5) {
      const politeReply =
        finalLanguage === "ar"
          ? `ÙŠØ¨Ø¯Ùˆ Ø£Ù† Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØªÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£ØµÙˆØ§Øª Ø¹Ø§Ù…Ø© (Ù…Ø«Ù„ ØµÙˆØª Ù…Ø­Ø±Ùƒ Ø£Ùˆ Ø¶ÙˆØ¶Ø§Ø¡) Ø¨Ø¯ÙˆÙ† ÙƒÙ„Ø§Ù… ÙˆØ§Ø¶Ø­ ÙŠÙ…ÙƒÙ†Ù†ÙŠ ÙÙ‡Ù…Ù‡.

Ø­ØªÙ‰ Ø£Ø³ØªØ·ÙŠØ¹ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¨Ø¯Ù‚Ø©ØŒ Ø£Ø±Ø¬Ùˆ Ù…Ù†Ùƒ ÙˆØ§Ø­Ø¯ Ù…Ù† Ø§Ù„Ø®ÙŠØ§Ø±ÙŠÙ†:
1. ØªØ³Ø¬ÙŠÙ„ Ù…Ù‚Ø·Ø¹ Ø¬Ø¯ÙŠØ¯ ØªØ´Ø±Ø­ ÙÙŠÙ‡ Ø¨ØµÙˆØªÙƒ Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© (Ù…Ø«Ù„: "ÙŠÙˆØ¬Ø¯ ØµÙˆØª Ø·Ù‚Ø·Ù‚Ø© Ø¹Ù†Ø¯ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø­Ø±Ùƒ ÙˆÙ‡Ùˆ Ø¨Ø§Ø±Ø¯"ØŒ Ø£Ùˆ "Ø§Ù„Ø³ÙŠØ§Ø±Ø© ØªÙ‡ØªØ² Ø¹Ù†Ø¯Ù…Ø§ Ø£ØªÙˆÙ‚Ù Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©").
2. Ø£Ùˆ ÙƒØªØ§Ø¨Ø© ÙˆØµÙ Ù‚ØµÙŠØ± Ù„Ù„Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø®Ø§Ù†Ø© Ø§Ù„Ù†Øµ.

ÙƒÙ„Ù…Ø§ ÙƒØ§Ù† Ø§Ù„ÙˆØµÙ Ø£Ø¯Ù‚ØŒ Ø§Ø³ØªØ·Ø¹Øª Ø£Ù† Ø£Ù‚Ø¯Ù‘Ù… Ù„Ùƒ ØªØ´Ø®ÙŠØµÙ‹Ø§ Ø£Ø¯Ù‚ ÙˆØ®Ø·ÙˆØ§Øª Ø¹Ù…Ù„ÙŠØ© Ù„Ù…Ø§ ÙŠÙ…ÙƒÙ†Ùƒ ÙØ­ØµÙ‡ Ø£Ùˆ Ù…Ù†Ø§Ù‚Ø´ØªÙ‡ Ù…Ø¹ Ø§Ù„Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ. ğŸš—ğŸ”`
          : `It seems that the voice note contains general sound (engine/noise) but no clear speech that I can understand.

To help you accurately, please either:
1. Record a new voice note where you *describe the problem in words* (for example: "there is a rattling noise on cold start" or "the car vibrates when I stop at a light"),  
2. Or type a short description of the problem in the text box.

The more details you share, the better I can guide you with likely causes and next steps. ğŸš—ğŸ”`;

      return res.status(200).json({
        reply: politeReply,
        language: finalLanguage,
        transcript: transcriptText,
      });
    }

    // ğŸ” Ù†Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù‚Ø±ÙŠØ¨Ø© Ù…Ù† auto_common_issues.json
    let matchedIssues = [];
    try {
      matchedIssues = await findRelevantIssues(transcriptText);
    } catch (e) {
      console.warn("findRelevantIssues failed:", e);
    }

    // Ù†Ø¬Ù‡Ø² Ù†Øµ JSON Ù„Ù„Ù…Ø´Ø§ÙƒÙ„ (Ø¥Ù† ÙˆØ¬Ø¯)
    const issuesJson =
      matchedIssues && matchedIssues.length
        ? JSON.stringify(matchedIssues, null, 2)
        : "[]";

    // ğŸ§¾ Ù†Ø¨Ù†ÙŠ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙŠ Ù†Ø±Ø³Ù„Ù‡Ø§ Ù„Ù€ GPT
    const userBundle = `
Voice transcription (what the user said, any language):
"""
${transcriptText}
"""

Additional note from user (if any):
"""
${note || "N/A"}
"""

Matched issues from auto_common_issues.json:
${issuesJson}

Please respond in the same language as the user (detected: ${finalLanguage}).
`;

    // ğŸ¤– Ù†Ø³ØªØ¯Ø¹ÙŠ Responses API Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ù†Ø³Ù‚
    const response = await openai.responses.create({
      model: "gpt-4.1-mini",
      input: [
        {
          role: "system",
          content: [
            {
              type: "input_text",
              text: BASE_SYSTEM_PROMPT,
            },
          ],
        },
        {
          role: "user",
          content: [
            {
              type: "input_text",
              text: userBundle,
            },
          ],
        },
      ],
    });

    const replyText =
      extractTextFromResponse(response) ||
      (finalLanguage === "ar"
        ? "Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØª. Ù…Ù† ÙØ¶Ù„Ùƒ Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„Ø§Ø­Ù‚Ø§Ù‹ Ø£Ùˆ Ø£Ø±Ø³Ù„ ÙˆØµÙØ§Ù‹ Ù…ÙƒØªÙˆØ¨Ø§Ù‹ Ù„Ù„Ù…Ø´ÙƒÙ„Ø©."
        : "An unexpected error occurred while analyzing the audio. Please try again later or send a written description of the problem.");

    return res.status(200).json({
      reply: replyText,
      language: finalLanguage,
      transcript: transcriptText,
    });
  } catch (err) {
    console.error("FixLens audio diagnose error:", err);

    // Ù†Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙØ§ØµÙŠÙ„ Ù…ÙÙŠØ¯Ø© Ù…Ù† Ø§Ù„Ø®Ø·Ø£
    const details =
      err?.response?.data ||
      err?.body ||
      err?.message ||
      "Unknown error in audio-diagnose API.";

    return res.status(500).json({
      error: "Audio diagnosis failed",
      details,
    });
  }
}
