// api/audio-diagnose.js
import OpenAI, { toFile } from "openai";
import { findRelevantIssues } from "../lib/autoKnowledge.js";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// ===================== Helpers =====================

// ØªØ®Ù…ÙŠÙ† Ø§Ù„Ù„ØºØ© Ù…Ù† Ø§Ù„Ù†Øµ Ø¥Ø°Ø§ Ù…Ø§ Ø¬Ø§Ø¡ Ø´ÙŠØ¡ Ù…Ù† Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
function guessLanguage(text) {
  if (!text || !text.trim()) return "en";
  const t = text.trim();

  // Arabic
  if (/[\u0600-\u06FF]/.test(t)) return "ar";
  // Russian
  if (/[\u0400-\u04FF]/.test(t)) return "ru";
  // Greek
  if (/[\u0370-\u03FF]/.test(t)) return "el";
  // CJK
  if (/[\u4E00-\u9FFF]/.test(t)) return "zh";
  if (/[\u3040-\u30FF]/.test(t)) return "ja";
  if (/[\u1100-\u11FF]/.test(t)) return "ko";

  return "en";
}

// Ù‡Ù„ Ø§Ù„ØªØ±Ø§Ù†Ø³ÙƒØ±Ø¨Øª ÙØ¹Ù„Ø§Ù‹ ÙŠØ´Ø¨Ù‡ ÙˆØµÙ Ù…Ø´ÙƒÙ„Ø© Ø³ÙŠØ§Ø±Ø©ØŸ
function looksLikeCarDescription(text) {
  if (!text) return false;
  const t = text.toLowerCase().trim();

  const words = t.split(/\s+/).filter((w) => /[a-z\u0600-\u06FF]/i.test(w));
  if (words.length < 3) return false;

  const carWords = [
    "engine",
    "motor",
    "car",
    "vehicle",
    "noise",
    "sound",
    "knock",
    "rattle",
    "click",
    "tapping",
    "vibration",
    "shake",
    "brake",
    "brakes",
    "belt",
    "timing",
    "chain",
    "transmission",
    "gear",
    "start",
    "starting",
    "idle",
    "rpm",
    "exhaust",
    "smoke",
    "leak",
    "oil",
    "coolant",
    "overheat",
    "overheating",

    "Ù…Ø­Ø±Ùƒ",
    "Ø§Ù„Ù…Ø­Ø±Ùƒ",
    "Ø³ÙŠØ§Ø±Ø©",
    "Ø§Ù„Ø³ÙŠØ§Ø±Ø©",
    "ØµÙˆØª",
    "Ø¶Ø¬ÙŠØ¬",
    "Ø·Ø±Ù‚",
    "Ø·Ù‚Ø·Ù‚Ø©",
    "Ø±Ø¬Ø©",
    "Ø±Ø¬Ù‡",
    "Ø§Ù‡ØªØ²Ø§Ø²",
    "ÙØ±Ø§Ù…Ù„",
    "Ø¨Ù†Ø²ÙŠÙ†",
    "Ø¯ÙŠØ²Ù„",
    "Ø¯Ø®Ø§Ù†",
    "ØªÙ‡Ø±ÙŠØ¨",
    "Ø²ÙŠØª",
    "Ù…Ø§Ø¡",
    "Ø­Ø±Ø§Ø±Ø©",
    "Ù‚ÙŠØ±",
    "Ø¬ÙŠØ±",
  ];

  const hasCarWord = carWords.some((w) => t.includes(w));
  return hasCarWord;
}

// Ø¨Ø±ÙˆÙ…Ø¨Øª Ø¹Ø§Ù… Ù„FixLens
const BASE_SYSTEM_PROMPT = `
You are **FixLens Auto**, an intelligent automotive diagnosis assistant.

You ONLY talk about vehicles (cars, SUVs, trucks, vans).
You NEVER talk about home appliances or non-vehicle devices.

You receive:
- A voice recording of a car-related sound (engine, belt, brakes, etc.).
- A short transcription of what the user said (if any).
- Optional extra text notes.
- Optional matched issues from auto_common_issues.json.

Your job:
1. LISTEN carefully to the audio: focus on the pattern of the sound (knocking, ticking, squealing, grinding, whining, etc.).
2. Combine what you hear + any transcript text + JSON hints.
3. Produce a clear diagnostic explanation.

Always be honest about uncertainty.

Your reply MUST follow this markdown structure:

**Quick Summary:**
- ...

**What the Sound Feels Like:**
- (e.g. "metallic knocking that follows engine speed", "high-pitched squeal on rotation", etc.)

**Most Likely Causes:**
1. ...
2. ...

**What You Can Check Now:**
- ...

**Safety / When to Stop Driving:**
- ...

**Next Professional Step:**
- ...

If the audio is too noisy or unclear, say so and ask the user for another recording or a text description instead of guessing.
`;

// Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Responses API
function extractTextFromResponse(resp) {
  try {
    const first = resp.output?.[0];
    if (!first || !first.content) return null;
    const textPart = first.content.find((c) => c.type === "output_text");
    if (textPart && textPart.text) return textPart.text.toString();
  } catch (e) {
    console.error("Failed to extract text from OpenAI response:", e);
  }
  return null;
}

// ===================== Handler =====================

export default async function handler(req, res) {
  if (req.method !== "POST") {
    res.setHeader("Allow", ["POST"]);
    return res
      .status(405)
      .json({ error: "Method not allowed. Use POST." });
  }

  try {
    const body =
      typeof req.body === "string" ? JSON.parse(req.body) : req.body || {};

    const {
      audioBase64,
      mimeType,
      language: clientLanguage, // Ù…Ù† Flutter (Ù…Ø«Ù„ "ar" Ø£Ùˆ "en")
      note,
    } = body;

    if (!audioBase64 || typeof audioBase64 !== "string") {
      return res.status(400).json({
        error: "Missing 'audioBase64' field (base64 string).",
      });
    }

    const audioBuffer = Buffer.from(audioBase64, "base64");

    // ğŸ”¹ Ù†Ø³ØªØ®Ø¯Ù… Whisper ÙÙ‚Ø· Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙŠ ÙƒÙ„Ø§Ù… Ù…Ø³Ù…ÙˆØ¹ (Ù„Ùˆ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØªÙƒÙ„Ù…)
    const file = await toFile(audioBuffer, "recording.m4a", {
      type: mimeType || "audio/m4a",
    });

    const transcription = await openai.audio.transcriptions.create({
      model: "whisper-1",
      file,
      ...(clientLanguage &&
      clientLanguage !== "auto" &&
      typeof clientLanguage === "string"
        ? { language: clientLanguage }
        : {}),
    });

    const transcriptText = (transcription.text || "").trim();
    console.log("FixLens audio transcript:", transcriptText);

    let finalLanguage =
      clientLanguage && clientLanguage !== "auto"
        ? clientLanguage
        : guessLanguage(transcriptText || note || "");

    // ğŸ”¹ Ù†Ø¬Ø±Ø¨ Ù†Ø¬ÙŠØ¨ Ù‚Ø¶Ø§ÙŠØ§ Ù‚Ø±ÙŠØ¨Ø© Ù…Ù† Ø§Ù„Ù€ knowledge base
    let matchedIssues = [];
    try {
      matchedIssues = await findRelevantIssues(
        transcriptText || note || ""
      );
    } catch (e) {
      console.warn("findRelevantIssues failed:", e);
    }

    const issuesJson =
      matchedIssues && matchedIssues.length
        ? JSON.stringify(matchedIssues, null, 2)
        : "[]";

    // ğŸ”¹ Ø§Ù„Ù€ bundle Ø§Ù„Ù†ØµÙŠ Ø§Ù„Ù„ÙŠ ÙŠØ±ÙˆØ­ Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù…Ø¹ Ø§Ù„ØµÙˆØª
    const userBundle = `
Transcription of the voice (if any words were detected):
"""
${transcriptText || "N/A"}
"""

User note (if any text was provided in the app):
"""
${note || "N/A"}
"""

Matched issues from auto_common_issues.json:
${issuesJson}

User language code (for your reply): ${finalLanguage}
`;

    // ğŸ”¥ Ù‡Ù†Ø§ Ø§Ù„Ø³Ø­Ø±: Ù†Ø±Ø³Ù„ Ø§Ù„ØµÙˆØª Ù†ÙØ³Ù‡ + Ø§Ù„Ù†Øµ Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„
    const response = await openai.responses.create({
      model: "gpt-4.1-mini",
      input: [
        {
          role: "system",
          content: [
            {
              type: "input_text",
              text: BASE_SYSTEM_PROMPT,
            },
          ],
        },
        {
          role: "user",
          content: [
            {
              type: "input_audio",
              audio: {
                data: audioBase64,
                format: (mimeType && mimeType.split("/").pop()) || "m4a",
              },
            },
            {
              type: "input_text",
              text: userBundle,
            },
          ],
        },
      ],
    });

    let replyText = extractTextFromResponse(response);

    if (!replyText) {
      replyText =
        finalLanguage === "ar"
          ? "Ø§Ø³ØªÙ„Ù…Øª Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØªÙŠ Ù„ÙƒÙ† Ù„Ù… Ø£Ø³ØªØ·Ø¹ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­. Ø±Ø¬Ø§Ø¡Ù‹ Ø£Ø¹Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ ÙÙŠ Ù…ÙƒØ§Ù† Ø£ÙƒØ«Ø± Ù‡Ø¯ÙˆØ¡Ø§Ù‹ Ø£Ùˆ Ø£Ø¶Ù ÙˆØµÙØ§Ù‹ Ù…ÙƒØªÙˆØ¨Ø§Ù‹ Ù„Ù„Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ø±Ø©."
          : "I received the voice note but couldn't clearly analyze the sound. Please try again in a quieter environment or add a short written description of the problem.";
    }

    return res.status(200).json({
      reply: replyText,
      language: finalLanguage,
      transcript: transcriptText,
      issues: matchedIssues || [],
      source: "fixlens-audio-waveform",
    });
  } catch (err) {
    console.error("FixLens audio diagnose error:", err);

    const details =
      err?.response?.data ||
      err?.body ||
      err?.message ||
      "Unknown error in audio-diagnose API.";

    return res.status(500).json({
      error: "Audio diagnosis failed",
      details,
    });
  }
}
