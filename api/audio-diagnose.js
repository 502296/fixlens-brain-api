// api/audio-diagnose.js
// FixLens Sound Lab â€“ Level 3 (Advanced car sound analysis)
// JSON body from Flutter (base64 audio)

import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Ù†ÙØ³ Ø¯Ø§Ù„Ø© ØªØ®Ù…ÙŠÙ† Ø§Ù„Ù„ØºØ© Ø§Ù„Ù„ÙŠ Ø§Ø³ØªØ®Ø¯Ù…Ù†Ø§Ù‡Ø§ Ø³Ø§Ø¨Ù‚Ø§Ù‹
function guessLanguage(text) {
  if (!text || !text.trim()) return null;
  const t = text.trim();

  if (/[\u0600-\u06FF]/.test(t)) return "ar"; // Arabic
  if (/[\u0400-\u04FF]/.test(t)) return "ru"; // Russian
  if (/[Ã¡Ã©Ã­Ã³ÃºÃ±Ã¼ÃÃ‰ÃÃ“ÃšÃ‘Ãœ]/.test(t)) return "es"; // Spanish-ish
  if (/[Ã¤Ã¶Ã¼ÃŸÃ„Ã–Ãœ]/.test(t)) return "de"; // German-ish
  if (/[Ã Ã¢Ã§Ã©Ã¨ÃªÃ«Ã®Ã¯Ã´Ã»Ã¹Ã¼Ã¿Ã€Ã‚Ã‡Ã‰ÃˆÃŠÃ‹ÃŽÃÃ”Ã›Ã™ÃœÅ¸]/.test(t)) return "fr"; // French-ish

  return "en";
}

// Ù†Ø­ÙˆÙ„ Ø§Ù„Ù…Ø§ÙŠÙ… ØªØ§ÙŠØ¨ Ø¥Ù„Ù‰ format Ù…Ù‚Ø¨ÙˆÙ„ Ù…Ù† gpt-audio
function mapMimeToFormat(mimeType) {
  const mt = (mimeType || "").toLowerCase();

  if (mt.includes("wav")) return "wav";
  if (mt.includes("mp3") || mt.includes("mpeg")) return "mp3";

  // âœ… IMPORTANT:
  // gpt-audio ÙŠÙ‚Ø¨Ù„ ÙÙ‚Ø·: "wav" Ø£Ùˆ "mp3"
  // Ø£ÙŠ Ø´ÙŠØ¡ Ø¢Ø®Ø± (m4a, webm, ...) Ù†Ø­ÙˆÙ„Ù‡ Ø¥Ù„Ù‰ "mp3"
  return "mp3";
}

export default async function handler(req, res) {
  if (req.method !== "POST") {
    res.setHeader("Allow", "POST");
    return res.status(405).json({ error: "Method not allowed" });
  }

  try {
    // Flutter Ø£Ø­ÙŠØ§Ù†Ø§Ù‹ ÙŠØ±Ø³Ù„ body ÙƒØ³Ù„Ø³Ù„Ø© Ù†ØµÙŠØ©
    let body = req.body;
    if (typeof body === "string") {
      try {
        body = JSON.parse(body);
      } catch {
        // Ø®Ù„ÙŠÙ‡Ø§ ÙØ§Ø¶ÙŠØ© Ù„Ùˆ ÙØ´Ù„ Ø§Ù„Ù€ JSON
        body = {};
      }
    }

    const audioBase64 = body?.audioBase64;
    const mimeType = body?.mimeType || "audio/m4a";
    const preferredLanguage = body?.language || "auto";

    if (!audioBase64) {
      return res.status(400).json({
        error: "No audioBase64 provided in request body",
      });
    }

    const format = mapMimeToFormat(mimeType);

    // ðŸ§  Ù‡Ù†Ø§ Ø§Ù„Ø³Ø­Ø± Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ â€“ gpt-audio
    const completion = await openai.chat.completions.create({
      model: process.env.FIXLENS_AUDIO_MODEL || "gpt-audio",
      modalities: ["text", "audio"],
      audio: { voice: "alloy", format: "wav" }, // Ù†Ø·Ù†Ù‘Ø´ Ø§Ù„ØµÙˆØª Ø§Ù„Ø®Ø§Ø±Ø¬ Ø­Ø§Ù„ÙŠØ§Ù‹
      messages: [
        {
          role: "system",
          content: `
You are **FixLens Auto â€“ Sound Lab v3**, a world-class AI mechanic
specialized in diagnosing car problems *purely from sound*.

You receive a recording from somewhere in or around a vehicle:
engine bay, exhaust, suspension, brakes, steering, or cabin.

Analyze the **waveform itself**, not just speech:

1. Decompose the sound:
   - Identify main patterns: knocking, pinging, tapping, ticking,
     squeaking, chirping, grinding, whining, humming, rattling, hissing,
     whooshing, rumbling, belt slapping, metallic clunking, etc.
   - Notice if the sound follows engine RPM, road speed, bumps, braking,
     turning the steering wheel, or gear shifts.

2. Perform a mechanic-style analysis:
   - Map patterns to: top-end / bottom-end engine, ignition/misfire,
     timing chain/belt, accessory belt/tensioner, exhaust leaks,
     wheel bearings, CV joints, suspension, brakes, drivetrain, mounts.

3. Return a list of most likely causes with approximate probabilities
   that roughly sum to 1.0.

4. Assess overall risk level (CRITICAL / HIGH / MEDIUM / LOW).

5. Give clear next steps for the driver.

LANGUAGE:
- If "preferredLanguage" is given (like "ar", "en", "es"), answer in it.
- If preferredLanguage = "auto", try to match the driver's spoken language.
- Tone: calm, friendly, confident â€“ like an experienced mechanic
  explaining to a normal driver.

If the audio is mostly silence, strong wind noise, or human conversation
with no clear mechanical sound, say that you are **not confident** and
explain what kind of recording would help.
          `.trim(),
        },
        {
          role: "user",
          content: [
            {
              type: "text",
              text:
                preferredLanguage && preferredLanguage !== "auto"
                  ? `This is a car sound recording. Analyze ONLY the mechanical sound and answer in language code: ${preferredLanguage}.`
                  : `This is a car sound recording from a vehicle. Analyze ONLY the mechanical sound (not my words) and reply in the same language as the driver if possible.`,
            },
            {
              type: "input_audio",
              input_audio: {
                data: audioBase64, // ðŸ‘ˆ base64 Ù…Ù† Flutter
                format,           // ðŸ‘ˆ mp3 Ø£Ùˆ wav ÙÙ‚Ø·
              },
            },
          ],
        },
      ],
    });

    const choice = completion.choices[0];
    let replyText = "";

    if (typeof choice.message.content === "string") {
      replyText = choice.message.content;
    } else if (Array.isArray(choice.message.content)) {
      const textPart = choice.message.content.find((p) => p.type === "text");
      replyText = textPart?.text || JSON.stringify(choice.message.content);
    } else {
      replyText = JSON.stringify(choice.message);
    }

    const detectedLang = guessLanguage(replyText);
    const finalLang =
      preferredLanguage && preferredLanguage !== "auto"
        ? preferredLanguage
        : detectedLang || "en";

    return res.status(200).json({
      reply: replyText,
      language: finalLang,
      source: "fixlens-audio",
    });
  } catch (apiError) {
    console.error("FixLens Sound Lab (gpt-audio) error:", apiError);

    return res.status(500).json({
      error: "Audio diagnosis failed",
      details:
        apiError?.response?.data ||
        apiError.message ||
        String(apiError),
    });
  }
}
