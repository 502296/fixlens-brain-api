// api/diagnose.js

const OpenAI = require("openai");



const client = new OpenAI({

  apiKey: process.env.OPENAI_API_KEY,

});



module.exports = async (req, res) => {

  if (req.method !== "POST") {

    res.status(405).json({ error: "Method not allowed" });

    return;

  }



  try {

    const {

      text,

      imageBase64,

      audioBase64,

      category,

      ui_language: uiLanguage,

    } = req.body || {};



    if (!text && !imageBase64 && !audioBase64) {

      res.status(400).json({ error: "Message, image or audio is required." });

      return;

    }



    // ðŸ§  ØªØ¹Ù„ÙŠÙ…Ø§Øª FixLens Brain

    const languageInstruction = uiLanguage

      ? `Answer in ${uiLanguage}.`

      : "Answer in the same language the user used.";



    const domainInstruction = category

      ? `Focus on this domain: ${category} (Auto / Home / Appliances).`

      : "";



    const systemText = `

You are **FixLens Brain**, an AI assistant that diagnoses real-world problems:

- Cars & vehicles

- Home & household issues

- Appliances & devices



Always be:

- Practical

- Clear

- Safe



STRUCTURE EVERY ANSWER LIKE THIS:



1. **FixLens Diagnosis** â€“ very short summary (2â€“3 lines).

2. **Recommended Steps** â€“ numbered list of clear, concrete steps.

3. **Important Safety Note** â€“ short paragraph about safety and when to stop and call a professional.



${languageInstruction}

${domainInstruction}



If you get an **image**, use it as a main clue in your reasoning.

If you get **audio**, assume it is a spoken description of the problem.

Never invent tools the user doesnâ€™t have. Work with simple tools and common sense.

    `.trim();



    // ðŸ‘¤ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ù†Øµ + ØµÙˆØ±Ø© + ØµÙˆØª)

    const userContent = [];



    if (text && text.trim().length > 0) {

      userContent.push({

        type: "input_text",

        text: text.trim(),

      });

    }



    if (imageBase64) {

      userContent.push({

        type: "input_image",

        image_url: `data:image/jpeg;base64,${imageBase64}`,

      });

    }



    if (audioBase64) {

      userContent.push({

        type: "input_audio",

        audio_url: `data:audio/m4a;base64,${audioBase64}`,

      });

    }



    // ðŸ“¡ Ù†Ø¯Ø§Ø¡ Responses API Ù…Ø¹ gpt-4o-mini

    const response = await client.responses.create({

      model: "gpt-4o-mini",

      input: [

        {

          role: "system",

          content: [

            {

              type: "input_text",

              text: systemText,

            },

          ],

        },

        {

          role: "user",

          content: userContent,

        },

      ],

      // Ù†ØºØ·ÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·ÙˆÙŠÙ„Ø© Ø¨Ù‡Ø§Ù…Ø´ Ù…Ø±ÙŠØ­

      max_output_tokens: 800,

    });



    let reply = "";



    if (

      response.output &&

      response.output[0] &&

      response.output[0].content &&

      response.output[0].content[0] &&

      response.output[0].content[0].text

    ) {

      reply = response.output[0].content[0].text.trim();

    }



    if (!reply) {

      reply = "FixLens Brain: empty reply.";

    }



    res.status(200).json({ reply });

  } catch (err) {

    console.error("FixLens Brain error:", err);

    res.status(500).json({

      error: "FixLens Brain: internal error.",

      details: String(err),

    });

  }

};
